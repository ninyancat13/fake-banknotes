---
title: "fake-banknotes"
author: "Amanda Efendi & Nina Kumagai"
date: "07/09/2019"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading the data

```{r}
df = read.table("data_banknote_authentication.txt",
                  sep=",",
                  col.names=c("variance", "skew", "kurtosis", "entropy", "class"),
                  fill=FALSE, 
                  strip.white=TRUE)
```

# Data Wrangling

```{r}
head(df)
```

```{r}
summary(df)
```

```{r}
library(dlookr)
describe_data = describe(df)
```

```{r}
#Change class to factor
df$class <- as.factor(df$class); str(df)

#Rename the levels of class
levels(df$class) <- c("Fake", "Real"); str(df)
```

# EDA on Data

```{r}
boxplot(df)
```

```{r}
hist(df)
```

```{r}
library(DataExplorer)
```

# Attempt to Split (?) - ask Ritu for confirmation

```{r}
# set.seed(7)
# ss <- sample(1:3,size=nrow(df),replace=TRUE,prob=c(0.333,0.333,0.333))
# sample1 <- df[ss==1,]
# sample2 <- df[ss==2,]
# sample3 <- df[ss==3,]
```

```{r}
# str(sample1)
# str(sample2)
# str(sample3)
```

```{r}
# sample_size <- nrow(df)
# set_proportions <- c(Training = 0.33, Validation = 0.33, Test = 0.33)
# set_frequencies <- diff(floor(sample_size * cumsum(c(0, set_proportions))))
# df$set <- sample(rep(names(set_proportions), times = set_frequencies))
```

# SPLIT INTO TEST AND TRAINING

```{r}
set.seed(1234)
TestIndex = sample((nrow(df)), floor(0.15*nrow(df)))
testing <- df[TestIndex,]
training <- df[-TestIndex,]; str(training)
```

# MACHINE LEARNING ALGORITHMS

## TREATMENT: Level 1 Logistic Regression

```{r}
library(pROC)
```

```{r}
glm = glm(class ~ . , data = training, family = binomial(link="logit"))
summary(glm)
```

```{r}
pred.glm = predict(glm, newdata=testing, type='response')
pred.glmclass = rep("Fake", length(pred.glm))
pred.glmclass[pred.glm>0.5] = "Real"

table(pred.glmclass, testing$class, dnn=c("Predictions","Actual"))
```

```{r}
# Probability of success???? 0.9854
(117+85)/(117+85+3)
```

**Logistic Regression** probability of success is 0.9853

```{r}
par(pty='s')
plot(roc(testing$class, pred.glm), legacy.axes=TRUE)
```


## TREATMENT: Level 2 Linar Discriminant Analysis

*insert codes here*


## TREATMENT: Level 3 Classification and Regression Trees

```{r fig.height=2, fig.width=3}
library(rpart)
rpart = rpart(class ~ ., data = training)
plot(rpart)
text(rpart)
```

```{r fig.height=4, fig.width=5}
library(partykit)
plot(as.party(rpart))
```

```{r}
rpart.pred = predict(rpart, newdata = testing, type = "class")
table(rpart.pred, testing$class, dnn = c("Prediction", "Actual"))
```

```{r}
# Probability of Success
(108+77)/(108+77+11+9)
```


## TREATMENT: Level 4 Naive Bayes

```{r}
library(e1071)
```

```{r}
nb =naiveBayes(class ~ ., data=training)
print(nb)
```

```{r}
printALL=function(model){
  trainPred=predict(model, newdata = training, type = "class")
  trainTable=table(training$class, trainPred)
  testPred=predict(nb, newdata=testing, type="class")
  testTable=table(testing$class, testPred)
  trainAcc=(trainTable[1,1]+trainTable[2,2])/sum(trainTable)
  testAcc=(testTable[1,1]+testTable[2,2])/sum(testTable)
  message("Contingency Table for Training Data")
  print(trainTable)
  message("Contingency Table for Test Data")
  print(testTable)
  message("Accuracy")
  print(round(cbind(trainAccuracy=trainAcc, testAccuracy=testAcc),3))
}
printALL(nb)
```



A more simple and elegant method?
```{r}
library(naivebayes)
```

```{r}
new_nb = naive_bayes(class ~ .,usekernel=T, data=training)
printALL(new_nb)
```

## TREATMENT: Level 5 Support Vector Machines

```{r}
plot(training)
```


```{r}
# Fitting SVM to the Training set 
library(e1071) 
  
svm_fit = svm(formula = class ~ ., 
                 data = training, 
                 type = 'C-classification', 
                 kernel = 'linear') 
```

```{r}
svm_pred = predict(svm_fit, newdata = testing) 
```

```{r}
cm = table(testing[,5], svm_pred) 
cm
```

```{r }
plot(svm_fit, training, variance ~ skew)
```

```{r}
plot(svm_fit, training, kurtosis ~ entropy)
```


```{r}
# # installing library ElemStatLearn 
# library(ElemStatLearn) 
#   
# # Plotting the training data set results 
# set = training
# X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01) 
# X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01) 
# # X3 = seq(min(set[, 3]) - 1, max(set[, 3]) + 1, by = 0.01) 
# # X4 = seq(min(set[, 4]) - 1, max(set[, 4]) + 1, by = 0.01) 
#   
# grid_set = expand.grid(X1, X2) 
# colnames(grid_set) = c('variance', 'skew') 
# y_grid = predict(svm, newdata = grid_set) 
#   
# plot(set[, -3:-5], 
#      main = 'SVM (Training set)', 
#      xlab = 'variance', ylab = 'skew', 
#      xlim = range(X1), ylim = range(X2)) 
#   
# contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE) 
#   
# points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine')) 
#   
# points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3')) 
```

